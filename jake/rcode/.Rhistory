#                    Train/Test Split
#---------------------------------------------------------------#
set.seed(0)
samp <- sample(length(y), size = floor(.7 * length(y)))
x_train <- x[samp, ]
y_train <- y[samp]
x_test <- x[-samp, ]
y_test <- y[-samp]
#---------------------------------------------------------------#
#                     Linear Regression
#---------------------------------------------------------------#
set.seed(0)
ols <- lm(log.SalePrice ~., data[samp, ])
ols.predict <- predict(ols, data[-samp, ])
ols.lRMSE <- lRMSE(y_test, ols.predict)
ols.lRMSE
mse(y_test, ols.predict)
ols.rmse <- rmse(y_test, ols.predict)
ols.rmse
ridge.rmse <- rmse(y_test, ridge.min.predict)
#Ridge with lambda.min
set.seed(0)
ridge.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = 0)
ridge.min.predict <- predict(ridge.cv, as.matrix(x_test), s = 'lambda.min')
ridge.min.rmse <- rmse(y_test, ridge.min.predict)
#Ridge with lambda.1se
set.seed(0)
ridge.1se.predict <- predict(ridge.cv, as.matrix(x_test), s = 'lambda.1se')
ridge.1se.rmse <- rmse(y_test, ridge.1se.predict)
ridge.min.rmse
ridge.1se.rmse
#---------------------------------------------------------------#
#                     Lasso Regression
#---------------------------------------------------------------#
#lasso with lambda.min
set.seed(0)
lasso.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = 1)
lasso.min.predict <- predict(lasso.cv, as.matrix(x_test), s = 'lambda.min')
lasso.min.rmse <- rmse(y_test, lasso.min.predict)
lasso.min.rmse
#lasso with lambda.1se
set.seed(0)
lasso.1se.predict <- predict(lasso.cv, as.matrix(x_test), s = 'lambda.1se')
lasso.1se.rmse <- rmse(y_test, lasso.1se.predict)
lasso.1se.rmse
#---------------------------------------------------------------#
#                     Random Forest Regression
#---------------------------------------------------------------#
set.seed(0)
rf <- randomForest(x_train, y_train)
rf.predict <- predict(rf, x_test)
rf.rmse <- rmse(y_test, rf.predict)
rf.rmse
tunRF(x_train, y_train)
rf.tune <- tuneRF(x_train, y_train)
rf.tune <- tuneRF(x_train, y_train, stepFactor=1.5, improve=1e-5, ntree=500)
rf.rmse
#---------------------------------------------------------------#
#                     Random Forest Regression
#---------------------------------------------------------------#
set.seed(0)
rf <- randomForest(x_train, y_train)
rf.predict <- predict(rf, x_test, mtry <- 67)
rf.rmse <- rmse(y_test, rf.predict)
rf.rmse
#---------------------------------------------------------------#
#                     ElasticNet Regression
#---------------------------------------------------------------#
#enet with lambda.min
set.seed(0)
enet.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = .5)
enet.min.predict <- predict(enet.cv, as.matrix(x_test), s = 'lambda.min')
enet.min.rmse <- rmse(y_test, enet.min.predict)
lasso.min.rmse
#enet with lambda.1se
set.seed(0)
enet.1se.predict <- predict(enet.cv, as.matrix(x_test), s = 'lambda.1se')
enet.1se.rmse <- enet(y_test, lasso.1se.predict)
enet.1se.rmse
#enet with lambda.1se
set.seed(0)
enet.1se.predict <- predict(enet.cv, as.matrix(x_test), s = 'lambda.1se')
enet.1se.rmse <- enet(y_test, enet.1se.predict)
enet.1se.rmse
enet.1se.rmse <- rmse(y_test, enet.1se.predict)
enet.1se.rmse
#---------------------------------------------------------------#
#                     ElasticNet Regression
#---------------------------------------------------------------#
#enet with lambda.min
set.seed(0)
enet.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = .5)
enet.min.predict <- predict(enet.cv, as.matrix(x_test), s = 'lambda.min')
enet.min.rmse <- rmse(y_test, enet.min.predict)
lasso.min.rmse
#---------------------------------------------------------------#
#                    Read in the data
#---------------------------------------------------------------#
set.seed(0)
data <- read.csv('transformed_data.csv')
data_train <- data[1:1460, -1]
x_train <- data[1:1460, 2:301]
y_train <- data[1:1460, 302]
View(x_train)
View(x_train)
#---------------------------------------------------------------#
#                        Libraries
#---------------------------------------------------------------#
library(glmnet)
library(EZtune)
library(randomForest)
library(elasticnet)
library(olsrr)
library(gbm)
#---------------------------------------------------------------#
#                        Libraries
#---------------------------------------------------------------#
library(glmnet)
library(EZtune)
library(randomForest)
library(elasticnet)
library(olsrr)
library(gbm)
#---------------------------------------------------------------#
#                    Read in the data
#---------------------------------------------------------------#
set.seed(0)
data <- read.csv('transformed_data.csv')
data_train <- data[1:1460, -1]
x_train <- data[1:1460, 2:301]
y_train <- data[1:1460, 302]
x_test <- data[1461:2919, 2:301]
x_train <- x_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001), ]
y_train <- y_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001)]
#---------------------------------------------------------------#
#                     Linear Regression
#---------------------------------------------------------------#
set.seed(0)
ols <- lm(log.SalePrice ~., data = data_train)
ols.predict <- predict(ols, x_test)
ols.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = exp(ols.predict))
write.table(ols.submission, './submissions/ols.submission.csv', row.names = FALSE, sep = ',')
cooks.distance(ols)
ols_plot_cooksd_chart(ols)
#Ridge with lambda.min
set.seed(0)
ridge.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = 0)
ridge.min.predict <- predict(ridge.cv, as.matrix(x_test), s = 'lambda.min')
ridge.min.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(ridge.min.predict)))
write.table(ridge.min.submission, 'ridge.min.submission.csv', row.names = FALSE, sep = ',')
#Ridge with lambda.1se
set.seed(0)
ridge.1se.predict <- predict(ridge.cv, as.matrix(x_test), s = 'lambda.1se')
ridge.1se.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(ridge.1se.predict)))
write.table(ridge.1se.submission, 'ridge.1se.submission.csv', row.names = FALSE, sep = ',')
#---------------------------------------------------------------#
#                     Lasso Regression
#---------------------------------------------------------------#
#lasso with lambda.min
set.seed(0)
lasso.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = 1)
lasso.min.predict <- predict(lasso.cv, as.matrix(x_test), s = 'lambda.min')
lasso.min.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(lasso.min.predict)))
write.table(lasso.min.submission, 'lasso.min.submission.csv', row.names = FALSE, sep = ',')
#lasso with lambda.1se
set.seed(0)
lasso.1se.predict <- predict(lasso.cv, as.matrix(x_test), s = 'lambda.1se')
lasso.1se.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(lasso.1se.predict)))
write.table(lasso.1se.submission, 'lasso.1se.submission.csv', row.names = FALSE, sep = ',')
#---------------------------------------------------------------#
#                     Random Forest Regression
#---------------------------------------------------------------#
set.seed(0)
rf <- randomForest(x_train, y_train)
rf.predict <- predict(rf, x_test)
rf.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(rf.predict)))
write.table(rf.submission, 'rf.submission.csv', row.names = FALSE, sep = ',')
#---------------------------------------------------------------#
#                     ElasticNet Regression
#---------------------------------------------------------------#
#enet with lambda.min
set.seed(0)
enet.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = .5)
enet.min.predict <- predict(enet.cv, as.matrix(x_test), s = 'lambda.min')
enet.min.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(enet.min.predict)))
write.table(enet.min.submission, 'enet.min.submission.csv', row.names = FALSE, sep = ',')
#enet with lambda.1se
set.seed(0)
enet.1se.predict <- predict(enet.cv, as.matrix(x_test), s = 'lambda.1se')
enet.1se.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(enet.1se.predict)))
write.table(enet.1se.submission, 'enet.1se.submission.csv', row.names = FALSE, sep = ',')
#---------------------------------------------------------------#
#                     Boosting Machine
#---------------------------------------------------------------#
set.seed(0)
gbm_train <- eztune(x_train, y_train, method = 'gbm', fast = TRUE)
gbm.predict <- predict(gbm_train$model, newdata = x_test, n.tree = gbm_train$n.trees)
gbm.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(gbm.predict)))
write.table(gbm.submission, 'gbm.submission.csv', row.names = FALSE, sep = ',')
#GBM fast = FALSE
set.seed(0)
gbm_train_slow <- eztune(x_train, y_train, method = 'gbm', fast = FALSE)
gbm.predict_slow <- predict(gbm_train_slow$model, x_test, n.tree = gbm_train_slow$n.trees)
gbm.submission_slow <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(gbm.predict_slow)))
write.table(gbm.submission, 'gbm.submission_slow.csv', row.names = FALSE, sep = ',')
#---------------------------------------------------------------#
#                     Boosting Machine
#---------------------------------------------------------------#
set.seed(0)
gbm_train <- eztune(x_train, y_train, method = 'gbm', fast = TRUE)
gbm.predict <- predict(gbm_train$model, newdata = x_test, n.tree = gbm_train$n.trees)
gbm.rmse <- rmse(y_test, gbm.predict)
gbm.rmse
#---------------------------------------------------------------#
#                        Libraries
#---------------------------------------------------------------#
library(glmnet)
library(EZtune)
library(randomForest)
library(elasticnet)
library(olsrr)
library(gbm)
#---------------------------------------------------------------#
#                    Read in the data
#---------------------------------------------------------------#
set.seed(0)
data <- read.csv('transformed_data.csv')
data_train <- data[1:1460, -1]
x_train <- data[1:1460, 2:301]
y_train <- data[1:1460, 302]
x_test <- data[1461:2919, 2:301]
x_train <- x_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001), ]
y_train <- y_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001)]
#---------------------------------------------------------------#
#                     Linear Regression
#---------------------------------------------------------------#
set.seed(0)
ls <- lm(log.SalePrice ~., data = data_train)
ls.predict <- predict(ls, x_test)
cooks.distance(ls)
ols_plot_cooksd_chart(ls)
library(ggplot2)
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_text(hjust = 0.5, size = 20))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank())
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 14))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12)) +
xlab('Observation Index')
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index')
#---------------------------------------------------------------#
#                     Boosting Machine
#---------------------------------------------------------------#
set.seed(0)
gbm.predict <- predict(gbm_train$model, newdata = x_test, n.tree = gbm_train$n.trees)
gbm.rmse <- rmse(y_test, gbm.predict)
gbm.rmse
#---------------------------------------------------------------#
#                    Read in the data
#---------------------------------------------------------------#
data <- read.csv('transformed_data.csv')
data <- data[1:1460, ]
data <- data[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001), ]
x <- data[, 2:(length(data[1,]) - 1)]
y <- data$log.SalePrice
#---------------------------------------------------------------#
#                    Train/Test Split
#---------------------------------------------------------------#
set.seed(0)
samp <- sample(length(y), size = floor(.7 * length(y)))
x_train <- x[samp, ]
y_train <- y[samp]
x_test <- x[-samp, ]
y_test <- y[-samp]
gbm.predict <- predict(gbm_train$model, newdata = x_test, n.tree = gbm_train$n.trees)
gbm.rmse <- rmse(y_test, gbm.predict)
gbm.rmse
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index') +
scale_y_continuous(breaks = seq(0, 0.3, .5), limits = c(0, 0.325))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index') +
scale_y_continuous(breaks = seq(0, 0.3, .5), limits = c(0, 0.5))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index') +
scale_y_continuous(breaks = seq(0, 0.3, .5), limits = c(0, 0.5),
labels = breaks)
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index') +
scale_y_continuous(breaks = seq(0, 0.3, .5), limits = c(0, 0.5),
labels = seq(0, 0.3, .5))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index') +
scale_y_continuous(limits = c(0, 0.5))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index') +
scale_y_continuous(limits = c(0, 0.4))
ols_plot_cooksd_chart(ls) +
theme(plot.title = element_blank(),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)) +
xlab('Observation Index') +
scale_y_continuous(limits = c(0, 0.35))
library(xgboost)
install.packages("xgboost")
library(Metrics)
library(xgboost)
xgb <- xgboost(data = x_train, label = y_train)
xgb <- xgboost(data = as.matrix(x_train), label = y_train)
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 25)
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 100)
xgb.predict <- predict(xgb, x_test)
xgb.predict <- predict(xgb, x_test)
xgb.predict <- predict(xgb, as.matrix(x_test))
gbm.rmse <- rmse(y_test, xbg.predict)
gbm.rmse <- rmse(y_test, xgb.predict)
gbm.rmse
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 25)
xgb.predict <- predict(xgb, as.matrix(x_test))
gbm.rmse <- rmse(y_test, xgb.predict)
gbm.rmse
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 500)
xgb.predict <- predict(xgb, as.matrix(x_test))
gbm.rmse <- rmse(y_test, xgb.predict)
gbm.rmse
cv <- xgb.cv(data = as.matrix(x_train), nrounds = 3, nthread = 2, nfold = 5, metrics = list("rmse"),
max_depth = 3, eta = 1, objective = "binary:logistic")
dtrain <- xgb.DMatrix(x_train, label = y_train)
dtrain <- xgb.DMatrix(x_train, label = as.matrix(y_train))
dtrain <- xgb.DMatrix(as.matrix(x_train), label = as.matrix(y_train))
cv <- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = list("rmse"),
max_depth = 3, eta = 1, objective = "binary:logistic")
param <- list(booster = "gblinear"
, objective = "reg:linear"
, subsample = 0.7
, max_depth = 5
, colsample_bytree = 0.7
, eta = 0.037
, eval_metric = 'mae'
, base_score = 0.012 #average
, min_child_weight = 100)
#---------------------------------------------------------------#
#                     xgboost
#---------------------------------------------------------------#
set.seed(0)
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 25,
params = param)
xgb.predict <- predict(xgb, as.matrix(x_test))
gbm.rmse <- rmse(y_test, xgb.predict)
gbm.rmse
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 25)
xgb.predict <- predict(xgb, as.matrix(x_test))
gbm.rmse <- rmse(y_test, xgb.predict)
gbm.rmse
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 250)
xgb <- xgboost(data = as.matrix(x_train), label = y_train, nrounds = 178)
xgb.predict <- predict(xgb, as.matrix(x_test))
gbm.rmse <- rmse(y_test, xgb.predict)
gbm.rmse
xgb.predict <- predict(xgb, as.matrix(x_test))
xgb.rmse <- rmse(y_test, xgb.predict)
xgb.rmse
#---------------------------------------------------------------#
#                     xgboost
#---------------------------------------------------------------#
set.seed(0)
params <- list(colsample_bytree = 0.4603, gamma = 0.0468,
learning_rate = 0.05, max_depth = 3,
min_child_weight = 1.7817, n_estimators = 2200,
alpha = 0.4640, lambda = 0.8571,
subsample = 0.5213)
#---------------------------------------------------------------#
#                        Libraries
#---------------------------------------------------------------#
library(glmnet)
library(EZtune)
library(randomForest)
library(elasticnet)
library(olsrr)
library(gbm)
library(ggplot2)
library(xgboost)
#---------------------------------------------------------------#
#                    Read in the data
#---------------------------------------------------------------#
set.seed(0)
data <- read.csv('transformed_data.csv')
data_train <- data[1:1460, -1]
x_train <- data[1:1460, 2:301]
y_train <- data[1:1460, 302]
x_test <- data[1461:2919, 2:301]
x_train <- x_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001), ]
y_train <- y_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001)]
#---------------------------------------------------------------#
#                     xgboost
#---------------------------------------------------------------#
set.seed(0)
params <- list(colsample_bytree = 0.4603, gamma = 0.0468,
learning_rate = 0.05, max_depth = 3,
min_child_weight = 1.7817, n_estimators = 2200,
alpha = 0.4640, lambda = 0.8571,
subsample = 0.5213)
xgb <- xgboost(data = as.matrix(x_train),
label = y_train,
params = params,
nrounds = 2200)
xgb.predict <- predict(xgb, as.matrix(x_test))
xgb.1se.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(xgb.predict)))
write.table(xgb.submission, 'xgb.submission.csv', row.names = FALSE, sep = ',')
xgb.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(xgb.predict)))
write.table(xgb.submission, 'xgb.submission.csv', row.names = FALSE, sep = ',')
#---------------------------------------------------------------#
#                     ElasticNet Regression
#---------------------------------------------------------------#
#enet with lambda.min
set.seed(0)
enet.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = .8)
enet.min.predict <- predict(enet.cv, as.matrix(x_test), s = 'lambda.min')
enet.min.submission <- data.frame('ID' = 1461:2919, 'SalePrice' = as.numeric(exp(enet.min.predict)))
write.table(enet.min.submission, 'enet.min.submission.csv', row.names = FALSE, sep = ',')
View(data)
View(data)
#---------------------------------------------------------------#
#                        Libraries
#---------------------------------------------------------------#
library(glmnet)
library(EZtune)
library(randomForest)
library(elasticnet)
library(olsrr)
library(gbm)
library(ggplot2)
library(xgboost)
#---------------------------------------------------------------#
#                    Read in the data
#---------------------------------------------------------------#
set.seed(0)
data <- read.csv('transformed_data.csv')
data_train <- data[1:1460, -1]
x_train <- data[1:1460, 2:301]
y_train <- data[1:1460, 302]
x_test <- data[1461:2919, 2:301]
x_train <- x_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001), ]
y_train <- y_train[-c(31, 89, 108, 333, 411, 524, 589, 826, 1001)]
#---------------------------------------------------------------#
#                     Lasso Regression
#---------------------------------------------------------------#
#lasso with lambda.min
set.seed(0)
lasso.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = 1)
lasso.cv <- cv.glmnet(as.matrix(x_train), as.matrix(y_train), alpha = 1)
plot(lasso.cv)
varImpPlot(rf)
varImpPlot(, type = 1)
varImpPlot(rf, type = 1)
rf <- randomForest(x_train, y_train, importance = TRUE, keep.forest = TRUE)
#---------------------------------------------------------------#
#                     Random Forest Regression
#---------------------------------------------------------------#
set.seed(0)
rf <- randomForest(x_train, y_train, importance = TRUE, keep.forest = TRUE)
varImpPlot(rf, type = 1)
#---------------------------------------------------------------#
#                        Libraries
#---------------------------------------------------------------#
library(glmnet)
library(EZtune)
library(randomForest)
library(elasticnet)
library(olsrr)
library(gbm)
library(ggplot2)
library(xgboost)
varImpPlot(rf)
plot(x_train$log.GrLivArea, y_train)
library(plotly)
plotly()
plot_ly()
plot_ly(plot(x_train$log.GrLivArea, y_train))
plot_ly(x_train$log.GrLivArea, y_train)
